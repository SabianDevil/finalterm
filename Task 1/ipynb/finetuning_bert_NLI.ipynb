{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oIBhzdVnoiv"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate accelerate scikit-learn matplotlib seaborn\n",
        "!pip install torch --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXSnIJ56lyJI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Otentikasi ke Hugging Face Hub untuk push to hub\n",
        "TOKEN = \"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\"\n",
        "login(token=TOKEN)\n",
        "\n",
        "# memeriksa ketersediaan CUDA untuk akselerasi komputasi matriks (Tensor Operations) karena jiika menggunakan CPU, proses fine-tuning akan berjalan lebih lambat.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Hardware Akselerator: {device}\")\n",
        "MODEL_CKPT = \"distilbert-base-uncased\"\n",
        "\n",
        "# Hyperparameter Training\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 3\n",
        "#    Learning Rate 2e-5 (0.00002) adalah standar emas untuk fine-tuning Transformer\n",
        "LEARNING_RATE = 2e-5\n",
        "MAX_LENGTH = 128\n",
        "\n",
        "# Sampling\n",
        "SAMPLE_SIZE = 5000\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDWo13BLmElN"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "print(\"\\n--- Memuat Dataset GLUE Benchmark (Subset MNLI) ---\")\n",
        "# MNLI (Multi-Genre Natural Language Inference) adalah dataset standar untuk menguji kemampuan logika model.\n",
        "dataset = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "id2label = {0: \"Entailment\", 1: \"Neutral\", 2: \"Contradiction\"}\n",
        "label2id = {\"Entailment\": 0, \"Neutral\": 1, \"Contradiction\": 2}\n",
        "NUM_LABELS = 3\n",
        "\n",
        "print(f\"Skema Label: {id2label}\")\n",
        "print(f\"\\nMelakukan Sub-sampling data ({SAMPLE_SIZE} sampel)...\")\n",
        "\n",
        "# Shuffle dengan SEED tetap untuk menjamin Reproducibility\n",
        "train_dataset = dataset[\"train\"].shuffle(seed=SEED).select(range(SAMPLE_SIZE))\n",
        "eval_dataset = dataset[\"validation_matched\"].shuffle(seed=SEED).select(range(1000))\n",
        "\n",
        "print(\"Dataset siap. Contoh data mentah:\", train_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkSZTgmamIPe"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "print(\"\\n--- Tokenisasi Input ---\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CKPT)\n",
        "\n",
        "def preprocess_nli(examples):\n",
        "    # NLI memerlukan DUA input: Premise dan Hypothesis.\n",
        "    # Tokenizer BERT/DistilBERT secara otomatis akan menyusun input menjadi: [CLS] Premise [SEP] Hypothesis [SEP] Token [SEP] sebagai separator agar tahu batas antar kalimat\n",
        "    return tokenizer(\n",
        "        examples[\"premise\"],\n",
        "        examples[\"hypothesis\"],\n",
        "        truncation=True,       # Memotong kalimat jika total token > 128\n",
        "        max_length=MAX_LENGTH\n",
        "    )\n",
        "\n",
        "# Mengaplikasikan fungsi preprocessing secara parallel\n",
        "tokenized_train = train_dataset.map(preprocess_nli, batched=True)\n",
        "tokenized_eval = eval_dataset.map(preprocess_nli, batched=True)\n",
        "\n",
        "# Membersihkan kolom teks mentah yang tidak dibutuhkan\n",
        "cols_to_remove = [\"premise\", \"hypothesis\", \"idx\"]\n",
        "tokenized_train = tokenized_train.remove_columns(cols_to_remove)\n",
        "tokenized_eval = tokenized_eval.remove_columns(cols_to_remove)\n",
        "\n",
        "# Rename kolom target agar sesuai ekspektasi Loss Function (CrossEntropyLoss) HuggingFace\n",
        "tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
        "tokenized_eval = tokenized_eval.rename_column(\"label\", \"labels\")\n",
        "\n",
        "# Konversi format list Python ke PyTorch Tensor\n",
        "tokenized_train.set_format(\"torch\")\n",
        "tokenized_eval.set_format(\"torch\")\n",
        "\n",
        "print(\"Preprocessing selesai. Struktur Input IDs sudah mengandung separator token.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdz6hZfFmMCb"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    # Mengambil indeks dengan probabilitas tertinggi\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    acc = accuracy.compute(predictions=predictions, references=labels)     # Accuracy: Metrik dasar untuk melihat persentase kebenaran global\n",
        "    f1_score = f1.compute(predictions=predictions, references=labels, average=\"weighted\")    # F1-Score: Digunakan karena dalam klasifikasi multi-kelas, perlu menyeimbangkan performa antar kelas terutama jika distribusi label data validasi tidak 100% seimbang\n",
        "\n",
        "    return {\"accuracy\": acc[\"accuracy\"], \"f1\": f1_score[\"f1\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HdxPFjbmOi9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "print(\"\\n--- Memuat Model Pre-trained ---\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_CKPT,\n",
        "    num_labels=NUM_LABELS,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "model.to(device) # Memindahkan model ke VRAM GPU\n",
        "\n",
        "repo_name = \"finetuning-distilbert-mnli\"\n",
        "\n",
        "# Konfigurasi Training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=repo_name,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    weight_decay=0.01,            # Regularisasi L2 untuk mencegah Overfitting\n",
        "    eval_strategy=\"epoch\",        # Evaluasi dilakukan setiap akhir epoch\n",
        "    save_strategy=\"epoch\",        # Checkpoint model disimpan setiap akhir epoch\n",
        "    load_best_model_at_end=True,  # Otomatis me-load model dengan performa validasi terbaik di akhir\n",
        "    push_to_hub=True,\n",
        "    report_to=\"none\",\n",
        "    logging_steps=50,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "print(\"Trainer siap dieksekusi.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpa3E_3jmQ3R"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Mulai Training\n",
        "print(\"Mulai Training MNLI...\")\n",
        "trainer.train()\n",
        "\n",
        "# Upload\n",
        "trainer.push_to_hub()\n",
        "print(\"Model ter-upload!\")\n",
        "\n",
        "# Visualisasi\n",
        "history = trainer.state.log_history\n",
        "steps = []\n",
        "losses = []\n",
        "\n",
        "for entry in history:\n",
        "    if \"loss\" in entry:\n",
        "        steps.append(entry[\"step\"])\n",
        "        losses.append(entry[\"loss\"])\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(steps, losses, label=\"Training Loss\", color=\"purple\", marker='o')\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Grafik Training MNLI (DistilBERT)\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QltwAgnTmVmo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import seaborn as sns\n",
        "\n",
        "def predict_logic(premise, hypothesis):\n",
        "    inputs = tokenizer(premise, hypothesis, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].cpu().numpy()\n",
        "\n",
        "    # Ambil prediksi tertinggi\n",
        "    pred_idx = probs.argmax()\n",
        "    pred_label = id2label[pred_idx]\n",
        "    pred_score = probs[pred_idx]\n",
        "\n",
        "    print(f\"\\nPremise:    {premise}\")\n",
        "    print(f\"Hypothesis: {hypothesis}\")\n",
        "    print(f\"ðŸ‘‰ Hubungan: {pred_label} ({pred_score:.2%})\")\n",
        "\n",
        "    # Visualisasi\n",
        "    plt.figure(figsize=(6, 3))\n",
        "    colors = [\"#2ca02c\", \"#7f7f7f\", \"#d62728\"]\n",
        "    sns.barplot(x=probs, y=list(id2label.values()), palette=colors)\n",
        "    plt.xlim(0, 1.1)\n",
        "    plt.title(\"Probabilitas Logika\")\n",
        "    plt.show()\n",
        "\n",
        "# Entailment\n",
        "predict_logic(\"A soccer player is running across the field.\", \"A person is moving.\")\n",
        "\n",
        "# Contradiction\n",
        "predict_logic(\"A man is inspecting the uniform of a figure in some East Asian country.\", \"The man is sleeping on the couch.\")\n",
        "\n",
        "# Neutral\n",
        "predict_logic(\"The product was launched in 2010.\", \"The product is very expensive.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}